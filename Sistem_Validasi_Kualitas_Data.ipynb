{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plz77/Data-Quality-Validation-System/blob/main/Sistem_Validasi_Kualitas_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from transformers import pipeline\n",
        "\n",
        "\"\"\"\n",
        "--------------------------------------------------------------------------------\n",
        "Modul Pelatihan: Kualitas Data (Data Quality) untuk AI\n",
        "--------------------------------------------------------------------------------\n",
        "Halo! Proyek ini berfokus pada \"Penyaringan Data\". Sebelum AI belajar, kita\n",
        "harus memastikan data yang ia baca bersih. Data yang kotor (sampah) akan\n",
        "membuat AI menjadi bodoh atau tidak akurat.\n",
        "\n",
        "Tujuan Proyek:\n",
        "Membangun sistem otomatis yang memvalidasi apakah sebuah teks layak masuk ke\n",
        "dalam dataset pelatihan (training set) atau harus dibuang.\n",
        "\n",
        "Istilah Penting:\n",
        "1. Noise Cleaning: Menghapus karakter yang tidak perlu (seperti emoji berlebih\n",
        "   atau simbol aneh).\n",
        "2. Heuristic Filter: Aturan logika (seperti panjang kata) untuk membuang teks\n",
        "   yang tidak bermakna.\n",
        "3. Data Integrity: Memastikan data unik dan memiliki struktur yang benar.\n",
        "--------------------------------------------------------------------------------\n",
        "\"\"\"\n",
        "\n",
        "class DataQualityGuard:\n",
        "    def __init__(self):\n",
        "        print(\"Sistem Data Quality Guard diaktifkan...\")\n",
        "        # Memuat model sentimen standar sebagai bagian dari pipeline validasi\n",
        "        self.sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Membersihkan teks dari karakter yang tidak perlu.\"\"\"\n",
        "        # Menghapus spasi berlebih\n",
        "        text = \" \".join(text.split())\n",
        "        # Normalisasi karakter (menghapus aksen)\n",
        "        text = \"\".join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
        "        return text\n",
        "\n",
        "    def is_high_quality(self, text):\n",
        "        \"\"\"Memeriksa apakah teks memenuhi standar kualitas (Akurasi Tinggi).\"\"\"\n",
        "\n",
        "        # 1. Cek Panjang Teks (Minimal 10 karakter agar bermakna)\n",
        "        if len(text) < 10:\n",
        "            return False, \"Ditolak: Teks terlalu pendek (Informasi tidak cukup).\"\n",
        "\n",
        "        # 2. Cek Karakter Berulang (Mendeteksi spam/spamming karakter)\n",
        "        if re.search(r'(.)\\1{3,}', text):\n",
        "            return False, \"Ditolak: Terdeteksi karakter berulang (Potensi spam).\"\n",
        "\n",
        "        # 3. Cek Rasio Kata (Memastikan teks memiliki struktur kalimat)\n",
        "        words = text.split()\n",
        "        if len(words) < 3:\n",
        "            return False, \"Ditolak: Bukan merupakan kalimat lengkap.\"\n",
        "\n",
        "        # 4. Cek Karakter Spesial Berlebihan\n",
        "        special_chars = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "        if len(special_chars) / len(text) > 0.3:\n",
        "            return False, \"Ditolak: Terlalu banyak simbol/karakter spesial.\"\n",
        "\n",
        "        return True, \"Lolos: Data berkualitas tinggi.\"\n",
        "\n",
        "    def process_dataset(self, raw_data):\n",
        "        \"\"\"Memproses sekumpulan data dan memisahkan data bersih vs kotor.\"\"\"\n",
        "        clean_batch = []\n",
        "        rejected_log = []\n",
        "\n",
        "        print(f\"\\nMulai memproses {len(raw_data)} data mentah...\\n\")\n",
        "\n",
        "        for item in raw_data:\n",
        "            original_text = item\n",
        "            cleaned = self.clean_text(original_text)\n",
        "            is_valid, reason = self.is_high_quality(cleaned)\n",
        "\n",
        "            if is_valid:\n",
        "                # Jika lolos validasi, hitung sentimen\n",
        "                sentiment = self.sentiment_model(cleaned)[0]\n",
        "                clean_batch.append({\n",
        "                    \"text\": cleaned,\n",
        "                    \"sentiment\": sentiment['label'],\n",
        "                    \"confidence\": f\"{sentiment['score']:.2%}\"\n",
        "                })\n",
        "            else:\n",
        "                rejected_log.append({\"text\": original_text, \"reason\": reason})\n",
        "\n",
        "        return clean_batch, rejected_log\n",
        "\n",
        "# --- Simulasi Penggunaan ---\n",
        "if __name__ == \"__main__\":\n",
        "    guard = DataQualityGuard()\n",
        "\n",
        "    # Dataset mentah yang bercampur antara data bagus dan sampah\n",
        "    raw_dataset = [\n",
        "        \"The teacher explained the concept of AI very clearly today.\",\n",
        "        \"pashduashduashduashduashduashduashdua\", # Sampah\n",
        "        \"I hate this class!!!!!! @#$%^&*\",        # Terlalu banyak simbol\n",
        "        \"Great lesson.\",                           # Terlalu pendek\n",
        "        \"Python is the most popular language for data science in 2025.\",\n",
        "        \"Learning is funnnnnnnnnn\",                # Karakter berulang\n",
        "        \"The curriculum is well-structured and easy to follow.\"\n",
        "    ]\n",
        "\n",
        "    valid_data, rejected_data = guard.process_dataset(raw_dataset)\n",
        "\n",
        "    print(\"--- DATA YANG LOLOS (SIAP TRAINING) ---\")\n",
        "    for data in valid_data:\n",
        "        print(f\"[PASSED] {data['text']} | Sentimen: {data['sentiment']} ({data['confidence']})\")\n",
        "\n",
        "    print(\"\\n--- DATA YANG DITOLAK (REJECTED) ---\")\n",
        "    for data in rejected_data:\n",
        "        print(f\"[REJECTED] {data['text']} | Alasan: {data['reason']}\")\n",
        "\n",
        "    total = len(raw_dataset)\n",
        "    print(f\"\\nRingkasan Kualitas: {len(valid_data)}/{total} data bersih ({(len(valid_data)/total):.0%})\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "R5yVgwLGUBpH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}